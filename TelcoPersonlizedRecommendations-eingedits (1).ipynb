{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b74ef3-7c28-46d9-998d-1c4338205cef",
   "metadata": {},
   "source": [
    "### Personalized Telco Plans Using Sagemaker and Anthropic's Claude 3.5 Sonnet on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291aed9a-151b-4855-b079-612ad6d717ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import io\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import sqlalchemy as sa\n",
    "from botocore.exceptions import ClientError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65de5bfa-8055-42f8-8cd4-ef74ed679b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize AWS Services and Set Up Data Paths\n",
    "\n",
    "# In this step, we initialize the necessary AWS services and set up the paths for our data files. This includes creating a SageMaker session, defining an S3 bucket, and initializing the Bedrock client.\n",
    "\n",
    "# Why this step is important:\n",
    "#- SageMaker session allows us to interact with SageMaker services\n",
    "#- S3 bucket is used for storing our training data and model artifacts\n",
    "#- Bedrock client is used for generating personalized recommendations later in the process\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Define the S3 bucket\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # Replace with your preferred region\n",
    ")\n",
    "\n",
    "# File paths for output CSV files\n",
    "customer_data_csv = 'customer_data.csv'\n",
    "plans_and_addons_csv = 'plans_and_addons.csv'\n",
    "\n",
    "# Create a SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Define the S3 bucket\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # Replace with your preferred region\n",
    ")\n",
    "\n",
    "# File paths for output CSV files\n",
    "customer_data_csv = 'customer_data.csv'\n",
    "plans_and_addons_csv = 'plans_and_addons.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033fec4d-b7ea-490e-b8bc-f5d5051aa569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Generate Synthetic Customer Data\n",
    "\n",
    "# In this step, we create a function to generate synthetic customer profiles and use it to create a dataset of 1000 customers. This data is then saved to a CSV file.\n",
    "\n",
    "# Why this step is important:\n",
    "#- Synthetic data allows us to test and develop our model without using real customer data\n",
    "#- It helps in creating a diverse dataset with various customer profiles and usage patterns\n",
    "#- Saving to CSV allows easy storage and retrieval of the data for future steps\n",
    "\n",
    "def generate_customer_profile():\n",
    "    # Function implementation here...\n",
    "\n",
    "# Generate and write customer data to CSV\n",
    "with open(customer_data_csv, 'w', newline='') as csvfile:\n",
    "    # CSV writing logic here...\n",
    "\n",
    "print(\"Customer data generated and saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9466e-431c-46e3-82ab-78b40f2ffb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Data Processing and Feature Engineering\n",
    "\n",
    "# In this step, we load the synthetic customer data, process it, and engineer features for our machine learning model. We also split the data into training and test sets.\n",
    "\n",
    "# Why this step is important:\n",
    "# Data processing ensures our data is in the correct format for model training\n",
    "# Feature engineering helps in creating more informative inputs for our model\n",
    "# Splitting the data allows us to train our model and then test its performance on unseen data\n",
    "\n",
    "# Load the synthetic customer data for modeling\n",
    "customer_data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Data processing and feature engineering\n",
    "# ... (data processing steps) ...\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X = customer_data[['avg_data_usage', 'avg_call_minutes', 'avg_sms_count', 'monthly_bill', 'loyalty_years']]\n",
    "y = customer_data['upgrade_plan']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the train and test data as CSV\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "test_data.to_csv('test.csv', index=False)\n",
    "\n",
    "print(\"Data split into training and validation sets and saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba460d07-a815-47ab-b27a-2c5d993fe49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Prepare Data for SageMaker XGBoost\n",
    "\n",
    "# In this step, we convert our data to the LIBSVM format, which is required by SageMaker's XGBoost algorithm. We then upload this data to S3 for use in training.\n",
    "\n",
    "### Why this step is important:\n",
    "#- LIBSVM format is efficient for storing sparse data and is required by SageMaker's XGBoost implementation\n",
    "#- Uploading to S3 makes the data accessible to SageMaker for training\n",
    "\n",
    "# Convert data to LIBSVM format\n",
    "train_libsvm_file = 'train.libsvm'\n",
    "test_libsvm_file = 'test.libsvm'\n",
    "dump_svmlight_file(X_train, y_train, train_libsvm_file, zero_based=False)\n",
    "dump_svmlight_file(X_test, y_test, test_libsvm_file, zero_based=False)\n",
    "\n",
    "# Upload LIBSVM files to S3\n",
    "train_input = sagemaker_session.upload_data(train_libsvm_file, bucket=s3_bucket, key_prefix='telco-recommendation/train')\n",
    "test_input = sagemaker_session.upload_data(test_libsvm_file, bucket=s3_bucket, key_prefix='telco-recommendation/test')\n",
    "\n",
    "print(\"Data uploaded to S3 for SageMaker.\")\n",
    "print(f\"Train data S3 path: {train_input}\")\n",
    "print(f\"Test data S3 path: {test_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461a8fb-74fe-44d1-ad75-421762f40400",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Configure XGBoost Hyperparameters\n",
    "\n",
    "#In this step, we set up the hyperparameters for our XGBoost model. These parameters control various aspects of the model's behavior during training.\n",
    "\n",
    "### Why this step is important:\n",
    "#- Hyperparameters significantly influence the model's performance and generalization ability\n",
    "#- Properly tuned hyperparameters can lead to better predictions and reduced overfitting\n",
    "\n",
    "# Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"4\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.7\",\n",
    "    \"objective\":\"reg:squarederror\",\n",
    "    \"num_round\":\"50\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f92d8-86b7-4423-9eb2-b8ab0e7ec0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Set Up SageMaker XGBoost Estimator\n",
    "\n",
    "#Here, we create a SageMaker estimator for the XGBoost algorithm. This estimator will be used to train our model on the data we prepared earlier.\n",
    "\n",
    "### Why this step is important:\n",
    "#- The estimator encapsulates the training process and model artifacts\n",
    "#- It allows us to specify the compute resources to be used for training\n",
    "#- It provides an interface to interact with the trained model later\n",
    "\n",
    "# Set an output path for the trained model\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'DEMO-xgboost-as-a-built-in-algo'\n",
    "output_path = f's3://{bucket}/{prefix}/telco-xgb-built-in-algo/output'\n",
    "\n",
    "# Construct a SageMaker estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1',\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    volume_size=5, # 5 GB\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61893c77-b12c-41ab-904a-d42f177b47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Train the XGBoost Model\n",
    "\n",
    "#In this step, we use the estimator to train our XGBoost model on the prepared data.\n",
    "\n",
    "### Why this step is important:\n",
    "#- This is where the actual learning happens - the model learns to predict customer plan upgrades based on their usage data\n",
    "#- The trained model can then be used to make predictions on new, unseen data\n",
    "\n",
    "# Execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': test_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbbc37-b5fe-47a5-94e3-9c358c9e0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Deploy and Evaluate the Model\n",
    "\n",
    "#After training, we deploy the model to a SageMaker endpoint and evaluate its performance using various metrics.\n",
    "\n",
    "### Why this step is important:\n",
    "#- Deploying the model makes it available for real-time predictions\n",
    "#- Evaluating the model helps us understand its performance and whether it meets our requirements\n",
    "#- Multiple metrics provide a comprehensive view of the model's strengths and weaknesses\n",
    "\n",
    "# Deploy the trained model to an endpoint\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predictor.predict(test_libsvm_data.getvalue(), initial_args={'ContentType': 'text/x-libsvm'})\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "precision = precision_score(y_test, binary_predictions)\n",
    "recall = recall_score(y_test, binary_predictions)\n",
    "roc_auc = roc_auc_score(y_test, deserialized_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n",
    "# Delete the endpoint to avoid unnecessary charges\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a473c-89ef-4754-aad4-37b7b803604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9: Generate Personalized Recommendations\n",
    "\n",
    "#In this step, we use Amazon Bedrock to generate personalized recommendations for customers based on their usage data and available plans.\n",
    "\n",
    "### Why this step is important:\n",
    "#- Personalized recommendations can significantly improve customer satisfaction and retention\n",
    "#- It demonstrates how machine learning models can be combined with natural language processing for practical applications\n",
    "\n",
    "def get_personalized_recommendation(customer_data, plan_addons):\n",
    "    # Function implementation here...\n",
    "\n",
    "# Generate recommendations for a sample of customers\n",
    "with open('customer_data.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        customer_data = {k: v for k, v in row.items()}\n",
    "        recommendation = get_personalized_recommendation(customer_data, plans_and_addons_dict)\n",
    "        if recommendation:\n",
    "            print(f\"Recommendation for {customer_data['name']}:\")\n",
    "            print(recommendation)\n",
    "            print(\"\\n\")\n",
    "       \n",
    "        # Break after 2 recommendations for this example\n",
    "        if recommendation_count >= 2:\n",
    "            break\n",
    "\n",
    "# Save recommendations to SQL Database\n",
    "recommendations_df.to_sql('recommendations', engine, if_exists='replace', index=False, dtype=dtype)\n",
    "\n",
    "# Save to CSV\n",
    "recommendations_df.to_csv('customer_recommendations.csv', index=False)\n",
    "\n",
    "# Save to JSON (DynamoDB format)\n",
    "with open('recommendations_dynamo.json', 'w') as f:\n",
    "    json.dump(recommendations_dynamodb, f)\n",
    "\n",
    "print(\"Recommendations saved in multiple formats.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
